{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import base64\n",
    "from bitarray import bitarray\n",
    "import mmh3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bloom_Filter():\n",
    "    def __init__(self, items_count, FP_probability, size, hash_count):\n",
    "        self.items_count = items_count\n",
    "        self.FP_probability = FP_probability\n",
    "        self.size = size\n",
    "        self.hash_count = hash_count\n",
    "        self.bit_array = bitarray(self.size)\n",
    "        self.bit_array.setall(0)\n",
    "\n",
    "    def add(self, item):\n",
    "        try:\n",
    "            List = [mmh3.hash(item, i) % self.size for i in range(self.hash_count)]\n",
    "            for i in List:\n",
    "                self.bit_array[i] = True\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def lookup(self, item):\n",
    "        # print(\"[INFO]. Type : \", type(item))\n",
    "        flag = False\n",
    "        if type(item) == list:\n",
    "            for it in item:\n",
    "                for i in range(self.hash_count):\n",
    "                    digest = mmh3.hash(it, i) % self.size\n",
    "                    if self.bit_array[digest] == True:\n",
    "                        flag = True\n",
    "        return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFINN = pd.read_csv(\"AFINN-111.txt\", sep=\"\\t\", header=None).drop_duplicates()\n",
    "AFINN = pd.concat([AFINN[AFINN[1] == -4], AFINN[AFINN[1] == -5]], axis=0).set_index(0).T.to_dict('list')\n",
    "\n",
    "items_count = len(AFINN)\n",
    "FP_probability = 0.001\n",
    "\n",
    "size = int(-(items_count * math.log(FP_probability))/(math.log(2)**2))\n",
    "hash_count = int((size / items_count) * math.log(2))\n",
    "\n",
    "bloom_obj = Bloom_Filter(items_count, FP_probability, size, hash_count)\n",
    "added_status = [bloom_obj.add(key) for key in AFINN.keys()]\n",
    "\n",
    "with open(\"./bloom.txt\", \"wb\") as bloom_file:\n",
    "    bloom_file.write(base64.b64encode(bloom_obj.bit_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/21 22:43:47 WARN Utils: Your hostname, samay resolves to a loopback address: 127.0.1.1; using 10.0.0.101 instead (on interface wlo1)\n",
      "22/11/21 22:43:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/11/21 22:43:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint(\"./checkpoint\")\n",
    "\n",
    "headline = ssc.socketTextStream(\"localhost\", 9999).window(10, 10).map(lambda line: line.split(' ')) #.map(lambda x: x.lower())\n",
    "# headline.pprint()\n",
    "\n",
    "filtered_bad_headline = headline.filter(bloom_obj.lookup).map(lambda list_of_words: \" \".join(list_of_words))\n",
    "filtered_bad_headline.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]\r\r[Stage 0:>                                                          (0 + 1) / 1]\r22/11/21 22:44:01 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/11/21 22:44:01 WARN BlockManager: Block input-0-1669088641600 replicated to only 0 peer(s) instead of 1 peers\n",
      "\r[Stage 0:>                  (0 + 1) / 1][Stage 1:>                  (0 + 1) / 1]\r\r                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-11-21 22:44:07\n",
      "-------------------------------------------\n",
      "A new guy running for mayor reveals his fuck ups accidentally. \n",
      "dr mitchell rosenthal phoenix house founder dies 87\n",
      "elizabeth holmes is sentenced more than 11 years theranos fraud\n",
      "elizabeth holmes be sentenced fraud trial\n",
      "study finds hempfed cows had traces thc their milk\n",
      "deforestation brings batborne virus home roost\n",
      "nan goldin laura poitras two artists one devastating film\n",
      "people with adhd face withdrawal adderall shortage continues\n",
      "uk pledges 119 billion fight diseases poor countries\n",
      "world population reaches 8 billion un says\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-11-21 22:44:17\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/21 22:44:17 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/11/21 22:44:17 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "22/11/21 22:44:17 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:171)\n",
      "\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/11/21 22:44:17 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.Error: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1148)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "Caused by: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\t... 2 more\n",
      "22/11/21 22:44:17 WARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.\n"
     ]
    }
   ],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e089109934dce670f12a5ea18b34c05b6e8a3024f228764a8dec3bc9b7f09ae7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
